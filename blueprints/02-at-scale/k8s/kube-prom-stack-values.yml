# Copyright (c) CloudBees, Inc.

# https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
# https://artifacthub.io/packages/helm/prometheus-community/prometheus
# https://artifacthub.io/packages/helm/grafana/grafana
# https://artifacthub.io/packages/helm/prometheus-community/alertmanager

alertmanager:
  alertmanagerSpec:
    nodeSelector:
      kubernetes.io/os: linux
prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: "otel-collector"
        static_configs:
          #OTEL collector own metrics https://opentelemetry.io/docs/collector/internal-telemetry/
          - targets: ["otel-collector-opentelemetry-collector:8888"]
          #OTEL collector exporter prometheus metrics
          - targets: ["otel-collector-opentelemetry-collector:8889"]
    serviceMonitorSelector:
      # Note: For all Service Monitors, use a common label
      matchLabels:
        release: kube-prometheus-stack
    nodeSelector:
      kubernetes.io/os: linux
prometheusOperator:
  nodeSelector:
    kubernetes.io/os: linux
  admissionWebhooks:
    patch:
      nodeSelector:
        kubernetes.io/os: linux
    deployment:
      nodeSelector:
          kubernetes.io/os: linux
additionalPrometheusRulesMap:
  rule-name:
    groups:
      - name: CloudBees CI Performance
        rules:
          - alert: JenkinsTargetsAreDown
            expr: up{container="jenkins"} == 0
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: "{{ $labels.pod }} target is down"
              description: "{{ $labels.pod }} target is down"
          - alert: JenkinsHealthScoreToLow
            expr: sum(jenkins_health_check_score) by (pod) < 1
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: "{{ $labels.pod }} has a health score lower than 100%"
              description: "{{ $labels.pod }} has a to low health score {{ $value }}"
          - alert: JenkinsVMMemoryRationTooHigh
            expr: sum(vm_memory_heap_usage) by (pod) > 0.85
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: "{{$labels.pod}} too high memory ration (above 85%)"
              description: "{{$labels.pod}} has a VM memory ratio of {{ $value }}"
          - alert: JenkinsNewOrRestarted
            expr: sum(vm_uptime_milliseconds) by (pod) / 3600000 < 1
            for: 3m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.pod }} has uptime less than 1 hour"
              description: "{{ $labels.pod }} has low uptime and was either restarted or is a new instance (uptime: {{ $value }} hours)"
      - name: CloudBees CI Plugins
        rules:
          - alert: JenkinsTooManyPluginsNeedUpdate
            expr: sum(jenkins_plugins_withUpdate) by (pod) > 3
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.pod }} too many plugins updates"
              description: "{{ $labels.pod }} has {{ $value }} plugins that require an update"
          - alert: JenkinsPluginsFailedToInstall
            expr: sum(jenkins_plugins_failed) by (pod) > 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "{{ $labels.pod }} failed to install plugins"
              description: "{{ $labels.pod }} has {{ $value }} plugins that failed to install"
      - name: CloudBees CI  Builds
        rules:
          - alert: JenkinsTooLowJobSuccessRate
            expr: (sum(jenkins_runs_success_total) by (pod) * 100 / sum(jenkins_runs_total_total) by (pod)) < 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "{{$labels.pod}} has low job success rate under 50%"
              description: "{{$labels.pod}} instance has less than 50% of jobs being successful"
          - alert: JenkinsTooManyJobsQueued
            expr: sum(jenkins_queue_size_value) by (pod) > 10
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.pod }} too many jobs queued"
              description: "{{ $labels.pod}} has {{ $value }} jobs in the queue"
          - alert: JenkinsTooManyJobsStuckInQueue
            expr: sum(jenkins_queue_stuck_value) by (pod) > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.pod }} too many jobs queued"
              description: "{{ $labels.pod }} has {{ $value }} jobs stuck in queue"
kube-state-metrics:
  nodeSelector:
    kubernetes.io/os: linux
grafana:
  nodeSelector:
    kubernetes.io/os: linux
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/certificate-arn: ${cert_arn}
      alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
      external-dns.alpha.kubernetes.io/hostname: ${grafana_hostname}
    hosts:
      - ${grafana_hostname}
  adminPassword: ${grafana_password}
  plugins:
    - grafana-lokiexplore-app
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki-gateway
          uid: lokiDatasource
          derivedFields:
          - datasourceUid: tempoDatasource
            matcherRegex: trace_id=(\w+)
            name: TraceID
            url: '$${__value.raw}'
        - name: Tempo
          type: tempo
          url: http://tempo:3100
          access: proxy
          uid: tempoDatasource
          jsonData:
            tracesToLogsV2:
              datasourceUid: lokiDatasource
            tracesToMetrics:
              datasourceUid: prometheus
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: "grafana-dashboards-kubernetes"
          orgId: 1
          folder: "Kubernetes"
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
        - name: "grafana-dashboards-cloudbees"
          orgId: 1
          folder: "CloudBees"
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/grafana-dashboards-cloudbees
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ""
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ""
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ""
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ""
    grafana-dashboards-cloudbees:
      prometheus-plugin-applications:
        url: https://raw.githubusercontent.com/cloudbees/terraform-aws-cloudbees-ci-eks-addon/observability/blueprints/02-at-scale/k8s/grafana-db-application.json
        token: ""
      prometheus-plugin-builds:
        url: https://raw.githubusercontent.com/cloudbees/terraform-aws-cloudbees-ci-eks-addon/observability/blueprints/02-at-scale/k8s/grafana-db-builds.json
